---
title: Interface and File Formats Guide
---

## Overview
A run requires 
- a prompt file
- either a reward file or an environment file

Prompt files contain the prompts the model will respond to. Reward files contains a function to evalutate the models responses and return a reward value which provide feedback that the model to learn from. Environment files contains an environment class with an initialization function and a step function which returns. 

**Reward Function vs. Environment Class**: Choose the reward function if you only need single step interactions. Choose the environment class if you need multistep interactions.

## Prompt Files Requirements
Prompt files must be in JSONL (JSON Lines) format, with each line containing a complete, valid JSON object. The file extension should be `.jsonl`.
Each line in your prompt file must include:
- `prompt`: An array of message objects, where each message has:
  - `role`: The role of the speaker (e.g., "system", "user", "assistant")
  - `content`: The actual message text

You can include additional fields that your reward function might need for evaluation. Common examples include:
- `expected_result`: The expected answer or output
- `difficulty`: A difficulty level indicator
- `category`: A categorization tag
- Any custom metadata your reward function requires

### Example Prompt File

```jsonl
{"prompt":[{"role":"system","content":"You are an expert at mathematics."},{"role":"user","content":"What is the value of this expression: { ((54 - 140 * 118 + 130) + 197) + 46 }? Think for as long as you want, and then give your answer inside of <answer></answer> XML tags."}],"expected_result":-16093}
{"prompt":[{"role":"system","content":"You are an expert at mathematics."},{"role":"user","content":"What is the value of this expression: { (56 + 141 + 74) + 135 }? Think for as long as you want, and then give your answer inside of <answer></answer> XML tags."}],"expected_result":406}
{"prompt":[{"role":"system","content":"You are an expert at mathematics."},{"role":"user","content":"What is the value of this expression: { (194 + 92 + 120) + 26 - 22 }? Think for as long as you want, and then give your answer inside of <answer></answer> XML tags."}],"expected_result":410}
```

### Best Practices
- Include a diverse distribution of prompts that represent your use case
  - Generally, aim for at least 100 distinct examples for effective training
- Consider including difficulty levels or categories if your task has natural groupings

## Reward Functions Requirements

**Requirements**: The reward function file must be a python file and contain a reward function with a signature that matches either

- `def reward_fn(completion: str, **kwargs) -> float` 

    or

- `def reward_fn(completion: str, **kwargs) -> tuple[float, Dict[str, float | str]]`

**Inputs**: 
- `completion`: A string - the model's response.
- `**kwargs`: Contains all fields from the corresponding JSONL line in the prompt file. For example, if your prompt line includes `{"prompt": [...], "expected_result": 42, "difficulty": "hard"}`, then your reward function will receive `expected_result=42` and `difficulty="hard"` as keyword arguments. This allows you to access any metadata needed for evaluation.

**Outputs**: There are two valid output formats. 
- returning a numerical score, the reward
- returning a tuple where the first element is the numerical reward and the second item is a dictionary of information to be tracked
  - Each key in the info dict should be a string and the corresponding value either a float or a string
  - If it's a float the average of this metric for each step will be displayed on a graph
  - If it's a string, for each step 5 randomly sampled examples (along with the prompt) will be displayed in a table.

Global variables can be used and declared outside of the reward function. If they are stateful the user must write thread safe code.

### Reward Design Principles
- **Varied Scores**: Ensure the reward function produces a range of values. If all responses get the same reward, the model cannot learn to distinguish good from bad.
- **Clear Signal**: Higher rewards should consistently indicate better performance
- **Robust Evaluation**: Handle edge cases and unexpected inputs gracefully
- **Low Hackability**: Make it difficult for the model to cheat the reward function

## Environment Classes

**Requirements**: The environment class file must be a python file and contain a environment class function that conforms to the following interface.

```python
class CustomEnv:
    def __init__(self, **kwargs) -> None:
        """Initialize the environment with prompt data."""
        pass
    
    def step(self, action: str) -> Dict:
        """
        Process an action and return the result.
        
        Returns:
            Dict with keys:
                - 'observation': str, the next observation
                - 'reward': float, the reward for this step
                - 'done': bool, whether the episode is complete
                - 'reward_info_dict': Optional[Dict], additional information to be plotted
        """
        pass
```

**`__init__` Inputs**: `kwargs` contains all fields from the corresponding JSONL line in the prompt file. (same as kwargs in the reward function formulation)
**`step` Inputs**: `action` is a string, the models response (same as completion in the reward function formulation)
**`step` Outputs**:`step` returns a dict containing the following
- `observation`: A string, the environments response to the action -- the environments response in a conversation with the llm. For example, in a game of 20 questions, the observation might be `yes` or `no` in response to the question posed in the action.
- `reward`: the numerical reward for that step
- `done`: a bool - true if the episode is complete, otherwise false
- `reward_info_dict`: a dictionary of information to be tracked (same as optional dictionary returned in the reward function formulation)
  - Each key in the info dict should be a string and the corresponding value either a float or a string
  - If it's a float the average of this metric for each step will be displayed on a graph
  - If it's a string, for each step 5 randomly sampled examples (along with the prompt) will be displayed in a table.

### Best Practices
For each prompt, an environment will be initialized. Then step will be called repeatedly with the llm's response to initial prompt, and any conversation history (previous responses and observations).
- The environment should not be able to never return done=True. Having a max number of steps is usually a good idea.
- the length of the llm input builds up over the episode. If it passes the max_input_length, it will fail. Increase input length, and decrease response, observation, and steps per episode to deal with this.
- All the same reward design principles also apply to the reward with the environment


## Optimizing the Reward Function/Environment Class
* Our infrastructure already handles parallel reward computation (and init/steps for environments), so don't worry about making things asyncronous.
* However, declaring expensive or reused resources (e.g. LLM api clients) as global variables outside the reward function/environment class can greatly speed up the computation.
