---
title: Creating Your First Run
description: Complete walkthrough for launching your first training run on RunRL
---

# Creating Your First Run

Step-by-step guide for launching your first successful reinforcement learning training run.

## Before You Start

Ensure you have completed [account setup](/python/installation) including:

- âœ… Email verification
- âœ… Billing information added
- âœ… Understanding of $80/hour cost

## Step 1: Prepare Your Training Data

### Prompt File Format

Create a JSONL file with training examples:

```json
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is 2+2?"}], "expected_result": "4"}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Explain gravity"}], "expected_result": "A fundamental force"}
```

### Simple Reward Function

Start with this basic reward function:

```python
def reward_fn(prompt, response, expected_result=None):
    """Basic reward function for testing"""
    
    # Too short responses get no reward
    if len(response.strip()) < 10:
        return 0.0
        
    # Too long responses get penalty  
    if len(response.strip()) > 500:
        return 0.3
        
    # Check if expected content is present
    if expected_result and expected_result.lower() in response.lower():
        return 1.0
        
    # Default reasonable score
    return 0.6
```

## Step 2: Navigate to Create Run

1. **Go to [Create Run](https://runrl.com/runs/create)**
2. **Note the $80/hr cost display** next to the launch button
3. **Choose your model** from the curated list

## Step 3: Upload Your Files

### Upload Prompts
1. **Drag and drop** your JSONL file into the upload area
2. **Wait for upload completion** and validation
3. **Review preview** to ensure format is correct

### Create Reward Function
1. **Select "Reward Function" mode**
2. **Paste your reward function code** into the editor
3. **Test with sample data** if available

## Step 4: Configure Training Settings

### Recommended First Run Settings
- **Completion Length**: 256 tokens (balance cost vs quality)
- **Epochs**: 1 (start conservatively)
- **Learning Rate Multiplier**: 1.0 (default setting)

### Skip Advanced Options
- **Tools**: Leave unchecked for first run
- **HuggingFace Token**: Only needed for gated models

## Step 5: Launch and Monitor

### Starting the Run
1. **Review all settings** one final time
2. **Click "Start Reinforcement Learning"**
3. **Note the cost reminder** ($80/hr)
4. **Automatically redirected** to run detail page

### Monitoring Progress

**Watch for status changes**:
- `PENDING` â†’ Run queued for provisioning
- `PROVISIONING` â†’ GPU instance being allocated  
- `SETTING_UP` â†’ Environment being prepared
- `READY` â†’ About to start training
- `RUNNING` â†’ ðŸ’¸ **Billing active** - $80/hour

**Key metrics to monitor**:
- **Reward curves** - Should trend upward
- **Policy loss** - Should decrease over time  
- **KL divergence** - Should stay reasonable
- **Cost accumulation** - Track spending in real-time

## Step 6: Understanding Your Results

### Successful Training Indicators
- **Reward improvement** - Final > initial rewards
- **Stable metrics** - Charts show clear trends
- **No errors** - Terminal logs are clean
- **Cost efficiency** - Reasonable cost per improvement

### When to Cancel
Cancel your run if:
- **No improvement** after 30+ minutes
- **Costs exceed budget** for your experiment  
- **Error messages** appear in terminal
- **Metrics plateau early** with no progress

## Step 7: Post-Run Analysis

After completion or cancellation:

1. **Review final metrics** in Weights & Biases
2. **Analyze cost efficiency** - improvement per dollar
3. **Document learnings** - what worked, what didn't
4. **Plan next iteration** - adjustments for next run

## Success Checklist

Your first run was successful if:

- [ ] **Completed without errors** or achieved target metrics
- [ ] **Cost stayed within budget** expectations
- [ ] **Metrics showed improvement** from baseline
- [ ] **You understand** the training process flow

## Common First-Run Issues

### Run Stuck in "PENDING"
**Cause**: High demand or billing issues  
**Solution**: Check billing setup, wait, or contact support

### No Metrics Appearing  
**Cause**: W&B integration not working
**Solution**: Refresh page, check for errors in terminal

### Reward Function Errors
**Cause**: Syntax errors or wrong function signature
**Solution**: Check function name is `reward_fn` with correct parameters

### High Costs, No Improvement
**Cause**: Poor reward function or difficult task
**Solution**: Simplify reward logic, reduce complexity

## Next Steps

After your first successful run:

1. **Experiment with different models** and see performance differences
2. **Try more complex reward functions** for nuanced training
3. **Scale up with larger datasets** and longer training
4. **Read [Examples](/python/examples)** for advanced techniques

## Need Help?

- **Search the docs** using Cmd/Ctrl+K
- **Check [Troubleshooting](/python/troubleshooting)** for common issues  
- **Review [Examples](/python/examples)** for working templates
- **Monitor [Dashboard](https://runrl.com/dashboard)** for system status

Ready to create your first run? [Launch now!](https://runrl.com/runs/create)
